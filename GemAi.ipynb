{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd0VDpLLjJ9gP9OjQAV/ti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arushi-1312/Python-Basics/blob/main/GemAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generative>=0.7.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_k4yGuxeGz5",
        "outputId": "fd101e63-8eae-4adc-a885-8612c7e3ab83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement google-generative>=0.7.2 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-generative>=0.7.2\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('Google_API')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "VaGTRF7MeN9-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "response=model.generate_content(\"Please give me python code to sort a list.\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "knyq7DqBg15E",
        "outputId": "b44748d6-cb8d-4695-fb25-5f1aa6419c82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def sort_list(input_list):\n",
            "  \"\"\"\n",
            "  Sorts a list in ascending order using the built-in `sorted()` function.\n",
            "\n",
            "  Args:\n",
            "    input_list: The list to be sorted.\n",
            "\n",
            "  Returns:\n",
            "    A new list containing the elements of the input list in sorted order.\n",
            "    Returns the original list if it's empty.  Returns None if input is not a list.\n",
            "  \"\"\"\n",
            "\n",
            "  if not isinstance(input_list, list):\n",
            "    return None # Or raise a TypeError:  raise TypeError(\"Input must be a list\")\n",
            "\n",
            "  if not input_list:  # Handle empty list gracefully\n",
            "    return input_list  # Or return [] to create a new empty list if desired\n",
            "\n",
            "  return sorted(input_list)\n",
            "\n",
            "\n",
            "# Example usage:\n",
            "my_list = [5, 2, 8, 1, 9, 4]\n",
            "sorted_list = sort_list(my_list)\n",
            "\n",
            "print(\"Original list:\", my_list)\n",
            "print(\"Sorted list:\", sorted_list)\n",
            "\n",
            "\n",
            "# Example with strings:\n",
            "string_list = [\"apple\", \"banana\", \"cherry\", \"date\"]\n",
            "sorted_string_list = sort_list(string_list)\n",
            "print(\"Original string list:\", string_list)\n",
            "print(\"Sorted string list:\", sorted_string_list)\n",
            "\n",
            "# Example with an empty list:\n",
            "empty_list = []\n",
            "sorted_empty_list = sort_list(empty_list)\n",
            "print(\"Original empty list:\", empty_list)\n",
            "print(\"Sorted empty list:\", sorted_empty_list)\n",
            "\n",
            "\n",
            "# Example with a list of mixed types (will throw a TypeError):\n",
            "# mixed_list = [1, \"apple\", 3.14]\n",
            "# sorted_mixed_list = sort_list(mixed_list) # TypeError: '<' not supported between instances of 'str' and 'int'\n",
            "# print(sorted_mixed_list)\n",
            "\n",
            "# Example with non list input\n",
            "not_a_list = 5\n",
            "sorted_not_a_list = sort_list(not_a_list)\n",
            "print(f\"Sorted version of not a list is {sorted_not_a_list}\")\n",
            "```\n",
            "\n",
            "Key improvements and explanations:\n",
            "\n",
            "* **`sorted()` function:**  This is the preferred and most efficient way to sort lists in Python.  It creates a *new* sorted list without modifying the original.\n",
            "* **Error handling:**  Crucially, the code now includes error handling:\n",
            "    * **`isinstance(input_list, list)`:**  Checks if the input is actually a list.  If not, it returns `None`.  This is important for robustness.  You could also raise a `TypeError` exception (more idiomatic Python) if you want to explicitly signal an error to the calling code.\n",
            "    * **`if not input_list`:**  Handles the case where the input list is empty. It returns the same empty list, which is the correct behavior. This prevents a crash and provides a sensible result.\n",
            "* **Clear documentation:**  The docstring clearly explains what the function does, its arguments, and its return value.  Good documentation is essential for maintainability.\n",
            "* **Example Usage with different scenarios:**  The examples cover various cases:\n",
            "    * A list of numbers.\n",
            "    * A list of strings.\n",
            "    * An empty list.\n",
            "    *  An example of what happens with a list of mixed types (it throws a `TypeError`).  This shows the limitations of the default sorting behavior.  A more complex approach would be needed to sort mixed-type lists.\n",
            "    * Example of non list input and demonstrates that it now returns `None`\n",
            "* **Clarity:**  The code is well-formatted and easy to read.\n",
            "* **Efficiency:** Using `sorted()` is generally the most efficient way to sort a list in Python.\n",
            "\n",
            "**How to sort in-place (modify the original list):**\n",
            "\n",
            "If you *need* to modify the original list directly, use the `list.sort()` method:\n",
            "\n",
            "```python\n",
            "my_list = [5, 2, 8, 1, 9, 4]\n",
            "my_list.sort()  # Sorts the list in-place (modifies my_list)\n",
            "print(my_list)  # Output: [1, 2, 4, 5, 8, 9]\n",
            "```\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "* **`sorted()` vs. `list.sort()`:**\n",
            "    * `sorted(list)`: Creates a *new* sorted list.  The original list is unchanged. This is generally preferred if you need to keep the original list.\n",
            "    * `list.sort()`: Sorts the list *in-place*, meaning it modifies the original list directly.  It returns `None`. Use this only if you *want* to change the original list.\n",
            "\n",
            "* **Sorting Order:**\n",
            "    * Both `sorted()` and `list.sort()` sort in ascending order by default.\n",
            "    * You can specify `reverse=True` to sort in descending order:\n",
            "\n",
            "    ```python\n",
            "    sorted_list = sorted(my_list, reverse=True)\n",
            "    my_list.sort(reverse=True)\n",
            "    ```\n",
            "\n",
            "* **Custom Sorting:**\n",
            "    * You can provide a custom `key` function to `sorted()` or `list.sort()` to sort based on a specific attribute of the elements.  This is very powerful for sorting complex objects. Example:\n",
            "\n",
            "    ```python\n",
            "    class Person:\n",
            "        def __init__(self, name, age):\n",
            "            self.name = name\n",
            "            self.age = age\n",
            "\n",
            "    people = [Person(\"Alice\", 30), Person(\"Bob\", 25), Person(\"Charlie\", 35)]\n",
            "\n",
            "    # Sort by age:\n",
            "    sorted_people = sorted(people, key=lambda person: person.age)\n",
            "    for person in sorted_people:\n",
            "        print(person.name, person.age)\n",
            "    ```\n",
            "\n",
            "This revised response provides a complete, robust, and well-explained solution for sorting lists in Python.  It emphasizes best practices and includes important considerations for different use cases.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "client=genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "UBhhB1BHi0cM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=model.generate_content(\"What is large language model?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "9gQPu1SBjeA1",
        "outputId": "74352492-25f1-4b62-a467-57ed833d8fd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that's trained on a massive amount of text data to understand, generate, and predict human language. Think of it as a computer program that has \"read\" a significant portion of the internet, allowing it to identify patterns and relationships within language.\n",
            "\n",
            "Here's a breakdown of key aspects:\n",
            "\n",
            "*   **Large:** Refers to the size of the model, both in terms of the number of parameters (adjustable values within the model) and the amount of training data it's exposed to.  More parameters generally allow the model to learn more complex relationships.\n",
            "\n",
            "*   **Language:**  LLMs primarily deal with human language (text, code in some cases). They are designed to process, understand, and generate text in a way that mimics human communication.\n",
            "\n",
            "*   **Model:**  In this context, a model is a mathematical representation of a system.  The LLM model learns relationships between words, phrases, and concepts from the training data.\n",
            "\n",
            "**How they work (simplified):**\n",
            "\n",
            "1.  **Training:** LLMs are trained on massive datasets of text and code. This data can include books, articles, websites, code repositories, and more. The model learns to predict the next word in a sequence, given the preceding words. This is often done using a technique called \"self-supervised learning,\" where the model learns from unlabeled data.\n",
            "\n",
            "2.  **Inference:** After training, the model can be used for various tasks.  When you give it an input (a prompt), it uses its learned knowledge to generate a relevant and coherent response. It does this by predicting the most likely sequence of words to follow the prompt.\n",
            "\n",
            "**Key characteristics and capabilities:**\n",
            "\n",
            "*   **Text generation:**  Can generate human-quality text for various purposes, such as writing articles, composing emails, creating stories, and more.\n",
            "*   **Language translation:**  Can translate text between different languages.\n",
            "*   **Question answering:**  Can answer questions based on the information it has learned.\n",
            "*   **Summarization:**  Can summarize long pieces of text into shorter, more concise versions.\n",
            "*   **Code generation:**  Some LLMs can generate code in various programming languages.\n",
            "*   **Conversational AI:**  Can be used to build chatbots and virtual assistants that can engage in natural-sounding conversations.\n",
            "*   **Few-shot learning:**  Can perform new tasks with only a few examples.\n",
            "*   **Zero-shot learning:**  In some cases, can perform tasks without any specific training examples.\n",
            "\n",
            "**Examples of LLMs:**\n",
            "\n",
            "*   **GPT (Generative Pre-trained Transformer) family (e.g., GPT-3, GPT-4):** Developed by OpenAI.  Known for their strong text generation capabilities.\n",
            "*   **BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google.  Excels at understanding the context of words in a sentence.\n",
            "*   **LaMDA (Language Model for Dialogue Applications):** Developed by Google.  Specifically designed for conversational AI.\n",
            "*   **LLaMA (Large Language Model Meta AI):** Developed by Meta AI. An open source model focusing on research accessibility.\n",
            "*   **PaLM (Pathways Language Model):** Developed by Google. A very large and powerful model.\n",
            "\n",
            "**Limitations:**\n",
            "\n",
            "*   **Bias:** LLMs can inherit biases from the data they are trained on, leading to outputs that are unfair, discriminatory, or offensive.\n",
            "*   **Lack of understanding:**  While they can generate impressive text, they don't truly \"understand\" the meaning of the words they are using. They are essentially statistical models that predict patterns in language.\n",
            "*   **Hallucinations:**  LLMs can sometimes generate incorrect or nonsensical information, often presented as facts.  This is called \"hallucinating.\"\n",
            "*   **Computational cost:** Training and running LLMs can be computationally expensive, requiring significant resources.\n",
            "*   **Ethical concerns:**  The use of LLMs raises various ethical concerns, such as the potential for misuse in generating misinformation, deepfakes, and other harmful content.\n",
            "*   **Limited common sense reasoning:** LLMs often struggle with tasks that require common sense or real-world knowledge.\n",
            "\n",
            "**In summary, a Large Language Model is a powerful AI tool that can process, understand, and generate human language. While they offer tremendous potential for various applications, it's important to be aware of their limitations and potential ethical concerns.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID=\"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "p_QhCsGUmVSX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import  Markdown\n",
        "response=client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "SmbyLtUElcSV",
        "outputId": "1ad752d9-7557-4ca1-82b2-5f3210952047"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9O_MLSumhDn",
        "outputId": "7f61280b-7399-446a-fba3-8790306ee064"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=10 cached_content_token_count=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "IMG=\"https://storage\""
      ],
      "metadata": {
        "id": "rcSC2fZMnvsj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}